下面是一套完整的 **LoRA 微调代码模板**，基于 HuggingFace 的 `transformers` + `peft` 框架，
适用于大多数主流大语言模型（如 LLaMA、ChatGLM、Baichuan 等）。你只需准备好模型和训练数据，
即可直接运行训练。

---

### ✅ 代码结构一览

```
lora_finetune/
├── train.py               # 主训练脚本
├── data/
│   └── train.json         # 训练数据（格式见下）
└── config/
    └── lora_config.json   # 可选的 LoRA 参数配置
```

---

## 🧠 1. 准备训练数据（data/train.json）

👇 使用类似 Alpaca 的指令格式：

```json
[
  {
    "instruction": "请根据温度和湿度预测产品质量。",
    "input": "温度: 75°C，湿度: 60%",
    "output": "预测质量为：合格。"
  },
  {
    "instruction": "预测以下工艺参数下的质量等级：",
    "input": "温度: 82°C，湿度: 85%，压力: 2.3bar",
    "output": "预测质量为：不合格。"
  }
]
```

---

## 🧩 2. 主训练脚本：`train.py`

```python
import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, TaskType

# ========== 参数配置 ==========
base_model_path = "path_to_your_local_model"   # 本地模型路径或HuggingFace模型名
data_path = "./data/train.json"
output_dir = "./lora_output"
use_8bit = True

# ========== 加载模型和 tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    load_in_8bit=use_8bit,
    torch_dtype=torch.float16
)

# ========== 配置 LoRA ==========
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# ========== 加载训练数据 ==========
def format_example(example):
    prompt = f"### 指令:\n{example['instruction']}\n### 输入:\n{example['input']}\n### 输出:\n{example['output']}"
    return {"text": prompt}

raw_dataset = load_dataset("json", data_files={"train": data_path})
dataset = raw_dataset["train"].map(format_example).map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length", max_length=512), batched=True)

# ========== 训练参数 ==========
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    save_strategy="epoch",
    logging_steps=10,
    report_to="none"
)

# ========== 开始训练 ==========
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)
)

trainer.train()
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
```

---

## 🧪 3. 微调完成后的推理示例

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

model = AutoModelForCausalLM.from_pretrained("path/to/base_model", load_in_8bit=True, device_map="auto")
model = PeftModel.from_pretrained(model, "./lora_output")
tokenizer = AutoTokenizer.from_pretrained("path/to/base_model")

input_text = "### 指令:\n请根据温度和湿度预测产品质量。\n### 输入:\n温度: 80°C，湿度: 70%\n### 输出:\n"

inputs = tokenizer(input_text, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## 🚀 一键运行提示

安装依赖：

```bash
pip install transformers peft datasets accelerate bitsandbytes
```

---

## 📌 支持的模型（部分需加载 `trust_remote_code=True`）

- ✅ LLaMA / LLaMA-2 / LLaMA-3
- ✅ ChatGLM / ChatGLM2 / ChatGLM3
- ✅ Baichuan 13B
- ✅ Mistral / Mixtral
- ✅ Qwen / Qwen-Chat
- ✅ InternLM / Yi / Zephyr

---
