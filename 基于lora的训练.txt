ä¸‹é¢æ˜¯ä¸€å¥—å®Œæ•´çš„ **LoRA å¾®è°ƒä»£ç æ¨¡æ¿**ï¼ŒåŸºäº HuggingFace çš„ `transformers` + `peft` æ¡†æ¶ï¼Œ
é€‚ç”¨äºå¤§å¤šæ•°ä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLaMAã€ChatGLMã€Baichuan ç­‰ï¼‰ã€‚ä½ åªéœ€å‡†å¤‡å¥½æ¨¡å‹å’Œè®­ç»ƒæ•°æ®ï¼Œ
å³å¯ç›´æ¥è¿è¡Œè®­ç»ƒã€‚

---

### âœ… ä»£ç ç»“æ„ä¸€è§ˆ

```
lora_finetune/
â”œâ”€â”€ train.py               # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train.json         # è®­ç»ƒæ•°æ®ï¼ˆæ ¼å¼è§ä¸‹ï¼‰
â””â”€â”€ config/
    â””â”€â”€ lora_config.json   # å¯é€‰çš„ LoRA å‚æ•°é…ç½®
```

---

## ğŸ§  1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆdata/train.jsonï¼‰

ğŸ‘‡ ä½¿ç”¨ç±»ä¼¼ Alpaca çš„æŒ‡ä»¤æ ¼å¼ï¼š

```json
[
  {
    "instruction": "è¯·æ ¹æ®æ¸©åº¦å’Œæ¹¿åº¦é¢„æµ‹äº§å“è´¨é‡ã€‚",
    "input": "æ¸©åº¦: 75Â°Cï¼Œæ¹¿åº¦: 60%",
    "output": "é¢„æµ‹è´¨é‡ä¸ºï¼šåˆæ ¼ã€‚"
  },
  {
    "instruction": "é¢„æµ‹ä»¥ä¸‹å·¥è‰ºå‚æ•°ä¸‹çš„è´¨é‡ç­‰çº§ï¼š",
    "input": "æ¸©åº¦: 82Â°Cï¼Œæ¹¿åº¦: 85%ï¼Œå‹åŠ›: 2.3bar",
    "output": "é¢„æµ‹è´¨é‡ä¸ºï¼šä¸åˆæ ¼ã€‚"
  }
]
```

---

## ğŸ§© 2. ä¸»è®­ç»ƒè„šæœ¬ï¼š`train.py`

```python
import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, TaskType

# ========== å‚æ•°é…ç½® ==========
base_model_path = "path_to_your_local_model"   # æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å
data_path = "./data/train.json"
output_dir = "./lora_output"
use_8bit = True

# ========== åŠ è½½æ¨¡å‹å’Œ tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    load_in_8bit=use_8bit,
    torch_dtype=torch.float16
)

# ========== é…ç½® LoRA ==========
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# ========== åŠ è½½è®­ç»ƒæ•°æ® ==========
def format_example(example):
    prompt = f"### æŒ‡ä»¤:\n{example['instruction']}\n### è¾“å…¥:\n{example['input']}\n### è¾“å‡º:\n{example['output']}"
    return {"text": prompt}

raw_dataset = load_dataset("json", data_files={"train": data_path})
dataset = raw_dataset["train"].map(format_example).map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length", max_length=512), batched=True)

# ========== è®­ç»ƒå‚æ•° ==========
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    save_strategy="epoch",
    logging_steps=10,
    report_to="none"
)

# ========== å¼€å§‹è®­ç»ƒ ==========
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)
)

trainer.train()
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
```

---

## ğŸ§ª 3. å¾®è°ƒå®Œæˆåçš„æ¨ç†ç¤ºä¾‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

model = AutoModelForCausalLM.from_pretrained("path/to/base_model", load_in_8bit=True, device_map="auto")
model = PeftModel.from_pretrained(model, "./lora_output")
tokenizer = AutoTokenizer.from_pretrained("path/to/base_model")

input_text = "### æŒ‡ä»¤:\nè¯·æ ¹æ®æ¸©åº¦å’Œæ¹¿åº¦é¢„æµ‹äº§å“è´¨é‡ã€‚\n### è¾“å…¥:\næ¸©åº¦: 80Â°Cï¼Œæ¹¿åº¦: 70%\n### è¾“å‡º:\n"

inputs = tokenizer(input_text, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ğŸš€ ä¸€é”®è¿è¡Œæç¤º

å®‰è£…ä¾èµ–ï¼š

```bash
pip install transformers peft datasets accelerate bitsandbytes
```

---

## ğŸ“Œ æ”¯æŒçš„æ¨¡å‹ï¼ˆéƒ¨åˆ†éœ€åŠ è½½ `trust_remote_code=True`ï¼‰

- âœ… LLaMA / LLaMA-2 / LLaMA-3
- âœ… ChatGLM / ChatGLM2 / ChatGLM3
- âœ… Baichuan 13B
- âœ… Mistral / Mixtral
- âœ… Qwen / Qwen-Chat
- âœ… InternLM / Yi / Zephyr

---
